{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f43818d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "Hello\n",
      "!\n",
      "¿\n",
      "Cómo\n",
      "estás\n",
      "?\n",
      "我很好\n",
      "，\n",
      "谢谢\n",
      "。\n",
      "مرحبا\n",
      "كيف\n",
      "حالك\n",
      "؟\n",
      "This\n",
      "is\n",
      "a\n",
      "test\n",
      "for\n",
      "multilingual\n",
      "tokenization\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def multilingual_tokenizer(text):\n",
    "    # Define patterns for multilingual tokenization\n",
    "    word_pattern = r\"\\w+\"\n",
    "    punctuation_pattern = r\"[^\\w\\s]\"\n",
    "    chinese_char_pattern = r\"[\\u4e00-\\u9fff]\"  # Matches Chinese characters\n",
    "    arabic_char_pattern = r\"[\\u0600-\\u06FF]\"   # Matches Arabic characters\n",
    "\n",
    "    # Combine patterns into a single regex\n",
    "    combined_pattern = f\"({word_pattern}|{punctuation_pattern}|{chinese_char_pattern}|{arabic_char_pattern})\"\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = re.findall(combined_pattern, text)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Test multilingual text\n",
    "text = \"\"\"\n",
    "Hello! ¿Cómo estás? 我很好，谢谢。مرحبا كيف حالك؟\n",
    "This is a test for multilingual tokenization.\n",
    "\"\"\"\n",
    "\n",
    "tokens = multilingual_tokenizer(text)\n",
    "\n",
    "# Output the tokens\n",
    "print(\"Tokens:\")\n",
    "for token in tokens:\n",
    "    print(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f558af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm\", 'going', 'to', 'https://karunya.edu', 'with', '@harleydavis', 'Email', 'me', 'at', 'harleydavis@karunya.edu', '#excited_withaimlstaff']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def tokenize_text(text):\n",
    "    # Define individual patterns\n",
    "    url_pattern = r\"https?://\\S+|www\\.\\S+\"\n",
    "    email_pattern = r\"\\b[\\w.-]+?@\\w+?\\.\\w+?\\b\"\n",
    "    hashtag_pattern = r\"#\\w+\"\n",
    "    mention_pattern = r\"@\\w+\"\n",
    "    contraction_pattern = r\"\\b\\w+'\\w+\\b\"\n",
    "    word_pattern = r\"\\b\\w+\\b\"\n",
    "\n",
    "    # Combine all patterns into a single regex\n",
    "    combined_pattern = f\"({url_pattern}|{email_pattern}|{hashtag_pattern}|{mention_pattern}|{contraction_pattern}|{word_pattern})\"\n",
    "\n",
    "    # Find all matches\n",
    "    tokens = re.findall(combined_pattern, text)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "text = \"I'm going to https://karunya.edu with @harleydavis Email me at harleydavis@karunya.edu #excited_withaimlstaff\"\n",
    "tokens = tokenize_text(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea57ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
